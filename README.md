# rahw

1. Клонируем себе репу, запускаем командой
```bash
docker compose up -d
```
После запуска у вас должна быть доступна постгря по порту **5432** и должен открываться интерфейс Airflow по **localhost:8080**

Посмотреть список контейнеров:
```bash
docker ps 
```
Почитать логи:
```bash
docker logs <container_id> 
```

2. Регистрируемся на https://api.nasa.gov/ , заполучаем ключ
3. Берем любой метод в котором в запросе можно прокинуть дату, например "Asteroids - NeoWs". Анализируем возращаемые данные. Определяемся с полями, в сулчае с "Asteroids - NeoWs" предлагаю взять следующие поля:
   * id
   * neo_reference_id
   * name
   * nasa_jpl_url
   * absolute_magnitude_h
   * estimated_diameter.meters.estimated_diameter_min
   * estimated_diameter.meters.estimated_diameter_max
   * metric_date - брать из "верхнего уровня" сообщения
   * load_date_time - дата\время загрузки данных
4. Пишем DDL под эти данные, создаем таблицу
5. Пилим даг со следующими требованиями:
   * даг должен запускаться ежедневно в 03:00 по Мск, начиная с 2023-11-01
   * первым делом даг должен проверять, является ли день месяца четным. Если день четный - уходим к конец процесса. Иначе:
   * запрашиваем данные из рестовой ручки сразу за два дня, сохраняем результат в файловую систему как есть ,в виде JSONa. 
   * читаем файл, пробегаемся по содержимому, вытягиваем из иерархии нужные поля в плоском виде
   * сохраняем результат в БД
   * опицонально - подймайте над тем, что произойдет если за один и тот же день процесс запутится несколько раз. Подумайте над тем как сделать красиво.
   * опционально - реализуйте задуманное в предыдущем пункте :)

# FAQ
### Q: С чего вообще начать ? Что почитать ? 
A: конечно же с документации: https://airflow.apache.org/docs/apache-airflow/stable/index.html

В особенности с базовых концепций: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.html

Есть книга https://www.amazon.com/Data-Pipelines-Apache-Airflow-Harenslak/dp/1617296902 , в целом весь базоый функционал покрывает. Читать лучше после core-concepts в доке и исключительно на английском языке. Данная книга в русском переводе является очень хорошим примером того, что литературу техническую надо читать на инглише :) Перевод отвратителен

### Q: При помощи чего реализовать ветвление (четный \ нечетный) ? 
A: Для этого необходимо использовать BranchPythonOperator, который импортируется так:
```
from airflow.operators.python import BranchPythonOperator
```
Подробнее: https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.BranchPythonOperator

### Q: А откуда дату брать? подойдет ли для этого пакет `datetime` и его функция `now()`? 
А: Нет, для этого **datetime не подойдет**. Потому что он возвращает дату\веремя из рельного мира (время на сервере). Это очень важный момент в идеологии Airflow, да и любого другого оркестратора процессов с возможностью перезупусать процессы в прошлом. Представим себе ситуацию, что на дворе 15 ноября 2023 года и мы захотели перезапустить как-то процесс за 01 октября 2023. Если мы будем использовать `datetime`, то оно нам вернет 2023-11-15, а нам надо, чтобы в прцоессе была дата 2023-10-01. 

### Q: Как же в итоге быть? Как получить дату, привязанную к запуску от конкретного числа? 
A: Пеобходимо привязываться к контексту запуска и использовать `ds` для получения даты равоной дате даграна. Или `prev_ds` для получения даты предыдущего рана, или же `next_ds` для следующего и т.д. В контексте вообще лежит много чего интересного, вот список того, что можно получить через макросы: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html

а вот пример того как использовать это в `BranchPythonOperator`:
```
 _check_odd_day = BranchPythonOperator(
     task_id='check_odd_day',
     provide_context=True,
     python_callable=check_odd_day
 )
```
Для версии Airflow 1.x очень важна строчка `provide_context=True`, она как раз таки и прокидывает этот контекст в функцию. Во второй версии Airflow (которая у нас сейчас используется) это уже не критично ибо оно автоматом делается. Но т.к. на бою у нас пока версия 1.10.х - мотаем на ус.
В общем где-то контест передаели, а потом его можно получить в функции, напрмимер:
```
def check_odd_day(**context):
    ds = context['ds']
```
В данной функции в переменной `ds` будет лежать значение равное дате из даграна. То есть даже если мы перезапустим какой-то процесс в прошлом, то тут будет лежать та дата от которой мы перезапускам процесс, а не текущая дата которая вернулась бы нам при помощи `datetime.now()`

### Q: Чем воспользоваться для получения данных из рестовой ручки ? 
А: для непосредственно получения данных испльзуем пакет `requests` + `PythonOperator`

### Q: У нас же есть ключ доступа к API, хранить его в открытом виде нельзя. Как быть, куда положить и как достать? 
А: Можно и нужно создать соединение в инерфейсе или через CLI, затем пароль (ключ в нашем случае) можно будет достать через `BaseHook` и его метод `get_connection`. Вопрос рассматривается тут:

https://stackoverflow.com/questions/45280650/store-and-access-password-using-apache-airflow

### Q: А стоит ли так делать ? 
А: Для учебных целей норм. На бою конечно для реализации какой-то логики типа нашей стоило бы написать кастомные хук и оператор. 

https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html

https://docs.astronomer.io/learn/what-is-a-hook

### Q: зачем ответ из ручки сохранять на файловую систему, нельзя ли сразу в БД всё сложить ?
А: Можно вообще всю логику упаковать в одну питонячую функцию и это даже может работать на малых объемах данных. Но вот представим себе, что у вас цепочка не из двух-трех шагов как у нас, а из 10. В случае, если всё упаковано в одну функцию и происходит ошибка где-то на одном из последних шагов (допустим сервер перезагрузили или ещё что), то при перезапуске процесса у вас все расчёты начнутся заново в функции. А если мы разделим всё на отдельные шаги (`Operator`), то мы сможем перезапускать процесс с определенного места, а не с самого начала, что сэкономит время и ресурсы.

### Q: Ну допустим. Почему именно сохранение в локальную файловую систему ? Есть ли аналоги? 
А: Да, есть и их, вероятнее всего, будет разумнее использовать. Мы используем локальную файловую систему потому что у нас шедулер крутится на одной машине и у нас есть гарантия того, что разные таски будут исполняться на одной и той же тачке, соедовательно и файловая система будет одна и та же. В случае если бы у нас был не `LocalExecutor`, а `CeleryExecutor` или `KubernetesExecutor`, то у нас этой гарантии не было бы, следовательно нам необходило было бы использовать для временного хранения папку на `FTP` или сетевую папку \ `NAS` 

### Q: Есть ли возможность обмениваться данными между тасками без сохранения информации на диск \ в БД? 
А: Да, есть. Механизм называется XCOM (https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html), который применим **только для маленького объема данных!**. То есть, условно, в одной таске сформировать какую-то строку в пару тысяч символов и передать в следующую таску через XCOM можно. Но это должна быть информация типа вы сформировали какой-то путь к файлу динамически или ещё что-то такое мелкое. Бизнес-данные передавать через XCOM не стоит. Ограничения на объем измеряются, условно, единицами мегабайт, а то и меньше.

### Q: Для взаимодействия с PostreSQL что исользовать ? 
A: Как бы это странно не прозвучало, но `PostgresOperator` или `JDBCOperator`. На самом деле для Airflow написано куча всякого разного и можно всегда начинать с того, чтобы загуглить что-то + Operator. С очень большой долей вероятности под это искомое кто-то уже написал хуки, сенсоры и операторы. Если же нет, то вы всегда можете наисать их самостоятельно. 
